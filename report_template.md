# Rapport de projet ‚Äî CSC8607 : Introduction au Deep Learning

> **Consignes g√©n√©rales**
>
> - Tenez-vous au **format** et √† l‚Äô**ordre** des sections ci-dessous.
> - Int√©grez des **captures d‚Äô√©cran TensorBoard** lisibles (loss, m√©triques, LR finder, comparaisons).
> - Les chemins et noms de fichiers **doivent** correspondre √† la structure du d√©p√¥t mod√®le (ex. `runs/`, `artifacts/best.ckpt`, `configs/config.yaml`).
> - R√©pondez aux questions **num√©rot√©es** (D1‚ÄìD11, M0‚ÄìM9, etc.) directement dans les sections pr√©vues.

---

## 0) Informations g√©n√©rales

- **√âtudiant¬∑e** : _DUMANGE, Valentine_
- **Projet** : _CUB-200-2011 (oiseaux, 200 esp√®ces) avec convolutions group√©es (grouped convolutions) (CUB-200-2011 √ó CNN 2D √† 3 stages)_
- **D√©p√¥t Git** : _[URL publique](https://github.com/valentinedum/csc8607_projects)_
- **Environnement** : `python == 3.10.18`, `torch == 2.5.1`, `cuda == 12.1`  
- **Commandes utilis√©es** :
  - Entra√Ænement : `python -m src.train --config configs/config.yaml`
  - LR finder : `python -m src.lr_finder --config configs/config.yaml`
  - Grid search : `python -m src.grid_search --config configs/config.yaml`
  - √âvaluation : `python -m src.evaluate --config configs/config.yaml --checkpoint artifacts/best.ckpt`

---

## 1) Donn√©es

### 1.1 Description du dataset

- **Source** (lien) : <https://huggingface.co/datasets/dpdl-benchmark/caltech_birds2011>
- **Type d‚Äôentr√©e** (image / texte / audio / s√©ries) : images et textes
- **T√¢che** (multiclasses, multi-label, r√©gression) : classification multiclasses
- **Dimensions d‚Äôentr√©e attendues** (`meta["input_shape"]`) : images de tailles vari√©es, apr√®s normalisation -> 224√ó224
- **Nombre de classes** (`meta["num_classes"]`) :  200 classes d‚Äôoiseaux

**D1.** Quel dataset utilisez-vous ? D‚Äôo√π provient-il et quel est son format (dimensions, type d‚Äôentr√©e) ?

Nous allons utiliser le dataset de huggingface nomm√© caltech_birds2011 (url ci-dessus). C'est un dataset avec des images d'oiseaux ainsi que leur espece (texte). Les images sont de tailles variables.

### 1.2 Splits et statistiques

| Split | #Exemples | Particularit√©s (d√©s√©quilibre, longueur moyenne, etc.) |
| ----: | --------: | ----------------------------------------------------- |
| Train |      4795 |         √©quilibr√©, pas de labels manquants,     images de tailles vari√©es                                      |
|   Val |      1199 |           √©quilibr√©, pas de labels manquants, iamges de tailles vari√©es                                           |
|  Test |      5794 |               d√©s√©quilibr√©,    pas de labels manquants , images de tailles vari√©es                                   |

**D2.** Donnez la taille de chaque split et le nombre de classes.  
Il y a 200 classes dans chaque split et le train fait 4795 lignes, le validation 1199, le test 5794.

**D3.** Si vous avez cr√©√© un split (ex. validation), expliquez **comment** (stratification, ratio, seed).
Dans mon dataset, j'avais initialement aucun set de validation. J'ai donc cr√©√© mon propre split √† partir du train en appliquant une stratification par classe avec un ratio de 80% train / 20% val.
La seed a √©t√© fix√©e √† 42 pour permettre la reproductibilit√©.

**D4.** Donnez la **distribution des classes** (graphique ou tableau) et commentez en 2‚Äì3 lignes l‚Äôimpact potentiel sur l‚Äôentra√Ænement.  
La distribution des classes montrent que les ensembles Train et Val sont tr√®s √©quilibr√©s, il ya autant d'√©chantillons (24) dans chaque classe. A l'inverse le dataset Test est moins √©quilibr√©. La plupart des classes ont 30 √©chantillons mais certains en ont moins avec 12 √©achntillons, ou une vingtaine.
Des graphiques ont √©t√© plot√© via tensorboard (voir le dossier runs).
![distribution_train](./artifacts/distribution_train.png)
![distribution_test](./artifacts/distribution_test.png)
![distribution_val](./artifacts/distribution_val.png)

**D5.** Mentionnez toute particularit√© d√©tect√©e (tailles vari√©es, longueurs variables, multi-labels, etc.).
Ce dataset a tr√®s peu de particularit√©s. Il n'a aucun label manquant. Ces images sont toutes en RGB mais sont par contres de tailles tr√®s diff√©rentes, on compte au moins 750 diff√©rentes tailles d'images.

### 1.3 Pr√©traitements (preprocessing) ‚Äî _appliqu√©s √† train/val/test_

Listez pr√©cis√©ment les op√©rations et param√®tres (valeurs **fixes**) :

- Vision : resize = [224, 224], center-crop = None, normalize = (mean=[0.48185426, 0.50031734, 0.42832923], std=[0.2270571, 0.2226704, 0.26213554])‚Ä¶
- Audio : resample = __Hz, mel-spectrogram (n_mels=**, n_fft=**, hop_length=__), AmplitudeToDB‚Ä¶
- NLP : tokenizer = __, vocab =__, max_length = __, padding/truncation =__‚Ä¶
- S√©ries : normalisation par canal, fen√™trage = __‚Ä¶

**D6.** Quels **pr√©traitements** avez-vous appliqu√©s (op√©rations + **param√®tres exacts**) et **pourquoi** ?
Comme indiqu√© en D5 , les images du dataset ont des tailles tr√®s vari√©s. Seulement es r√©seaux de neurones convolutifs ont besoin d'une entr√©e √† taille fixe. Nous redimensionnons donc les images √† [224, 224]. Apr√®s le resizing, il est important de transformer l'image en tenseur pour pouvoir la traiter avec pytorch. Puis nous normalisons notre tenseur avec mean=[0.48185426, 0.50031734, 0.42832923] et std=[0.2270571, 0.2226704, 0.26213554] car apr√®s analyse ce sont les statistiques que nous avons √† propos du dataset d'entrainement. Autrement nous aurions pu trouver sur internet les param√®tres moyens des datasets d'images connues tels que ImageNet et approximer par ceux-ci.
NB: Je n'ai pas fait de center-crop car un redimensionnement de l'image avait d√©j√† √©t√© fait. On ne voudrait pas qu'une partie de l'oiseau soit accidentellement coup√©e.

**D7.** Les pr√©traitements diff√®rent-ils entre train/val/test (ils ne devraient pas, sauf recadrage non al√©atoire en val/test) ? Tous mes pr√©traitements sont les m√™mes pour train, val et test pour √™tre sur que l'√©valuation sera repr√©sentative. Toutefois train recevra en plus de stransformations d'augmentation de donn√©es.

### 1.4 Augmentation de donn√©es ‚Äî _train uniquement_

- Liste des **augmentations** (op√©rations + **param√®tres** et **probabilit√©s**) :
  - ex. Flip horizontal p=0.5, RandomResizedCrop scale=**, ratio=** ‚Ä¶
  - Audio : time/freq masking (taille, nb masques) ‚Ä¶
  - S√©ries : jitter amplitude=**, scaling=** ‚Ä¶

**D8.** Quelles **augmentations** avez-vous appliqu√©es (param√®tres pr√©cis) et **pourquoi** ?  
Nous avons appliqu√© √† notre dataset plusieurs augmentations car nous avons un grand risque de surapprentissage avec tr√®s peu d'images par classe (24 environ). Pour que le mod√®le devienne plus robuste, nous appliquons un randomHorizontalFlip de probabilit√© 0.5 car un oiseau est le m√™me qu'il soit tourn√© vers la droite ou la gauche. Nous allons ainsi rendre le mod√®le invariant √† cela.
Nous allons aussi prendre en compte le fait que les photos ont pu √™tre prises sous diff√©rentes conditions d'√©clairage. Les param√®tres Variations al√©atoires de luminosit√© (¬±20%), contraste (¬±20%), saturation (¬±20%) et teinte (¬±10%) ont √©t√© choisi d'ap√®s la documentation de pytorch.
Enfin nous allons aussi appliquer des petites rotations (+-15 degr√©s) car les oiseaux peuvent √™tre plus ou moins inclin√©s. On rendra ainsi le mod√®le robuste aux changements d'orientation.

**D9.** Les augmentations **conservent-elles les labels** ? Justifiez pour chaque transformation retenue.
Oui, les transformations conservent les labels, c'est bien le plus important. Le mod√®le doit comprendre qui oiseau qu'il ait la t√™te √† droite ou √† gauche, qu'il ait √©t√© pris plus ou moins au soleil et plus ou moins inclin√© est le m√™me. Il garde la m√™me esp√®ce.

### 1.5 Sanity-checks

- **Exemples** apr√®s preprocessing/augmentation (ins√©rer 2‚Äì3 images/spectrogrammes) :

> ![original_0](./artifacts/original_0.png)
> ![augmented_0](./artifacts/augmented_0.png)
> ![preprocessed_0](./artifacts/preprocessed_augmented_0.png)

**D10.** Montrez 2‚Äì3 exemples et commentez bri√®vement.  
Les 3 images au-dessus sont de haut en bas:

- l'image originale
- l'image apr√®s augmentation
- l'image apr√®s augmentation puis preprocessing

Avec cet exemple, nous remarquons en effet que l'image avait une chance sur deux de connaitre une sym√©trie horizontale lors de l'augmentation. Ca n'a pas √©t√© le cas. Sinon, elle a un petit peu chang√© en termes de couleurs et a connu une petite rotation (<15 degr√®s en effet). L'augmentation semble avoir bien fonctionn√©.

Puis le preprocessing a normalis√© les couleurs de l'image d'o√π le changement marquant et surtout on a redimensionn√© l'image afin qu'elle soit en format carr√© de 224 par 224. Le preprocessing a lui aussi bien fonctionn√©.

D'autres images sont disponibles dans le dossier : `artifacts`

**D11.** Donnez la **forme exacte** d‚Äôun batch train (ex. `(batch, C, H, W)` ou `(batch, seq_len)`), et v√©rifiez la coh√©rence avec `meta["input_shape"]`.
D'apr√®s la sortie de mon script de test dans data_loading, la forme exact de sortie d'un batch est (64, 3, 224, 224). Ce qui est coh√©rent avec le "batch_size" et "input_shape" inscrit dans les configs.

---

## 2) Mod√®le

### 2.1 Baselines

**M0.**

- **Classe majoritaire** ‚Äî M√©trique : `Accuracy` ‚Üí score = `0.52%`
- **Pr√©diction al√©atoire uniforme** ‚Äî M√©trique : `Accuracy` ‚Üí score = `0.41%`  
_Ces scores tr√®s faibles (proches de 1/200 = 0.5%) confirment que le dataset est √©quilibr√© et ne pr√©sentent pas de biais. Notre mod√®le devra faire mieux que ces scores en apprenant des caract√©ristiques discriminantes_

### 2.2 Architecture impl√©ment√©e

- **Description couche par couche** (ordre exact, tailles, activations, normalisations, poolings, r√©siduels, etc.) :
  - Input ‚Üí 3√ó224√ó224
  - Initialization : Conv 3√ó3 (64 canaux, padding 1) ‚Üí BatchNorm ‚Üí ReLU.
  - Stage 1 (r√©p√©ter 2 fois) :
    - Conv 3√ó3 (64 ‚Üí 64, padding 1) ‚Üí BatchNorm ‚Üí ReLU.
    - Conv 3√ó3 (64 ‚Üí 64, padding 1, groups = G) ‚Üí BatchNorm ‚Üí ReLU.
  - MaxPool 2√ó2
  - Stage 2 (r√©p√©ter 2 fois) :
    - Conv 3√ó3 (64 ‚Üí 128, padding 1) ‚Üí BatchNorm ‚Üí ReLU.
    - Conv 3√ó3 (128 ‚Üí 128, padding 1, groups = G) ‚Üí BatchNorm ‚Üí ReLU.
  - MaxPool 2√ó2
  - Stage 3 (r√©p√©ter 2 fois) :
    - Conv 3√ó3 (128 ‚Üí 256, padding 1) ‚Üí BatchNorm ‚Üí ReLU.
    - Conv 3√ó3 (256 ‚Üí 256, padding 1, groups = G) ‚Üí BatchNorm ‚Üí ReLU.
  - Global Average Pooling
  - T√™te lin√©aire (256 ‚Üí 200) ‚Üí logits (dimension = nb classes)

Remarque : pour que groups=G soit valide, le nombre de canaux de la convolution doit √™tre divisible par G.

- **Loss function** :
  - Multi-classe : CrossEntropyLoss
  - Multi-label : BCEWithLogitsLoss
  - (autre, si votre t√¢che l‚Äôimpose)

- **Sortie du mod√®le** : forme = **(64, 200)**

- **Nombre total de param√®tres** : `1974088`

**M1.** D√©crivez l‚Äô**architecture** compl√®te et donnez le **nombre total de param√®tres**.  
Expliquez le r√¥le des **2 hyperparam√®tres sp√©cifiques au mod√®le** (ceux impos√©s par votre sujet).
L'architecture est un r√©seau de neurones convolutif (CNN) divis√© en trois √©tages principaux, o√π chaque √©tage est constitu√© d'une suite de blocs r√©p√©tant des op√©rations de convolution, de normalisation (BatchNorm) et d'activation non-lin√©aire (ReLU). La taille des images diminue progressivement gr√¢ce √† des couches de MaxPool, jusqu'√† une agr√©gation finale par moyenne (Average Pooling) avant la classification.
Ce r√©seau totalise 1 964 088 param√®tres entra√Ænables. Concernant les hyperparam√®tres impos√©s, l'utilisation de convolutions group√©es (G=2) permet de diviser les connexions entre canaux pour r√©duire le co√ªt de calcul et √©viter le surapprentissage, tandis que le nombre de blocs par stage (N=2) joue sur la profondeur du r√©seau pour permettre l'apprentissage de motifs plus ou moins complexes.

### 2.3 Perte initiale & premier batch

- **Loss initiale attendue** (multi-classe) ‚âà `-log(1/num_classes) = 5.2983`
- **Observ√©e sur un batch** : `5.3390`
- **V√©rification** : backward OK, gradients ‚â† 0

**M2.** Donnez la **loss initiale** observ√©e et dites si elle est coh√©rente. Indiquez la forme du batch et la forme de sortie du mod√®le.
La loss initiale est de 5.3390, ce qui est coh√©rent avec la loss th√©orique (tir√©e de la loi uniforme) = 5.2983.
Le batch d'entr√©e est de taille (64, 3, 224, 224), ce qui confirme que le mod√®le traite bien un batch de 64 images RVB de taille 224x224. La sortie du mod√®le (64, 200) correspond bien √† un loggit avec 200 classes

---

## 3) Overfit ¬´ petit √©chantillon ¬ª

- **Sous-ensemble train** : `N = 16` exemples
- **Hyperparam√®tres mod√®le utilis√©s** (les 2 √† r√©gler) : `Nombre de groupes G = 2`, `Nombre de blocs par stage = 2`
- **Optimisation** : LR = `0.001`, weight decay = `0.0` (0 ou tr√®s faible recommand√©)
- **Nombre d‚Äô√©poques** : `50`

>![train/loss](./artifacts/Train_Loss.svg)

**M3.** Donnez la **taille du sous-ensemble**, les **hyperparam√®tres** du mod√®le utilis√©s, et la **courbe train/loss** (capture). Expliquez ce qui prouve l‚Äôoverfit.
Nous avons pris un sous ensemble de 16 images avec les hyperparam√®tres Nombre de groupes = 2 et nombre de bloc par stage = 2.
Avec la courbe de train/loss du dessus, on comprend qu'au bout d'√† peine 20 epochs, le mod√®le ne fait quasi plus d'erreur (loss~=0). Le mod√®le connait "par coeur" le sous ensemble, il fait preuve d'overfit

---

## 4) LR finder

- **M√©thode** : balayage LR (log-scale), quelques it√©rations, log `(lr, loss)`
- **Fen√™tre stable retenue** : `1e-7 ‚Üí 1e-3`
- **Choix pour la suite** :
  - **LR** = `8e-4`
  - **Weight decay** = `1e-4` (valeurs classiques : 1e-5, 1e-4)

**Courbe de la loss en fonction du step**
> ![lr_finder_loss](./artifacts/lr_finder_loss.svg)

**Courbe du learning rate en fonction du step**
> ![lr_finder_lr](./artifacts/lr_finder_lr.svg)

**M4.** Justifiez en 2‚Äì3 phrases le choix du **LR** et du **weight decay**.
Le learning rate retenu est celui qui optimise la loss soit pour un weight decay de 1e-4. On peut le placer √† 1e-3 pour rester dans la fen√™tre de stabilit√©.
J'ai fait le lr_finder avec un weight decay de 1e-5 mais la loss optimale est la m√™me

---

## 5) Mini grid search (rapide)

- **Grilles** :
  - LR : `{0.0005, 0.001, 0.005}`
  - Weight decay : `{0.0, 0.0005}`
  - Hyperparam√®tre mod√®le A (num_blocks) : `{2, 3}`
  - Hyperparam√®tre mod√®le B (groups) : `{2, 4}`

- **Dur√©e des runs** : `3` √©poques par run, m√™me seed

| Run | LR | WD | num_blocks | groups | Val accuracy | Val loss | Notes |
| --- | --- | --- | --- | --- | --- | --- | --- |
| `proj22_lr=0.0005_bs=32_wd=0.0_blk=2_grp=2` | 0.0005 | 0.0 | 2 | 2 | **0.0275** | **4.9895** | üèÜ **MEILLEUR R√âSULTAT**: Convergence tr√®s stable (5.28‚Üí5.10‚Üí5.05), vitesse rapide, pas d'overfitting observ√© et meilleure accuracy et loss. |
| `proj22_lr=0.0005_bs=32_wd=0.0005_blk=2_grp=2` | 0.0005 | **0.0005** | 2 | 2 | 0.0259 | 5.0245 | **Tr√®s stable.** Le WD ajoute une r√©gularisation, mais r√©duit l√©g√®rement la pr√©cision. |
| `proj22_lr=0.0005_bs=64_wd=0.0005_blk=2_grp=2` | 0.0005 | 0.0005 | 2 | 2 | 0.0259 | 5.0336 | **Impact nul du Batch Size.** Passer √† BS=64 n'am√©liore pas vraiment les performances par rapport √† BS=32. |
| `proj22_lr=0.0005_bs=32_wd=0.0_blk=2_grp=4` | 0.0005 | 0.0 | 2 | **4** | 0.0209 | 5.0123 | **Sur-r√©gularisation.** `groups=4` ralentit la convergence et fait chuter l'accuracy. |
| `proj22_lr=0.0005_bs=32_wd=0.0_blk=3_grp=2` | 0.0005 | 0.0 | **3** | 2 | 0.0150 | 5.0743 | **Trop complexe.** `blk=3` ajoute trop de param√®tres. Convergence lente et chute massive d'accuracy (-45%). |

> Il n'y a pas de capture tensorboard tout simplement parce que j'ai l'impression qu'il y a un probleme UI dans la fen√™tre HParams. Mes metriques d'accuracy et de loss ne s'affiche pas alors que leurs colonnes existent et que les autres m√©triques aussi. J'ai regard√© sur internet et √ßa √† l'air d'√™tre un probleme des nouvelles versions de tensorboard. Comme mes r√©sultats sont quand m√™me affich√©s dans scalar, je les ai r√©cup√©r√©es ici. `test_tb/py` ne fonctionnait pas de m√™me.

**M5.** Pr√©sentez la **meilleure combinaison** (selon validation) et commentez l‚Äôeffet des **2 hyperparam√®tres de mod√®le** sur les courbes (stabilit√©, vitesse, overfit).
La meilleure combinaison est le Run 1 (LR=0.0005, BS=32, WD=0.0, Blocks=2, Groups=2). Il atteint la plus haute Accuracy de Validation (2.75%) mais aussi la plus basse Validation Loss (4.9895) apr√®s 3 epochs.
Concernant les impacts des hyperparam√®tres sur le mod√®le, les voici:

1. blk (nombre de blocs)
  On remarque qu'en augmentant le nombre de blocs la stabilit√© se d√©grade l'accuracy aussi (-45%). Le mod√®le devient trop complexe √† optimiser pour ce dataset (underfitting)et donc la convergence est lente.
2. groups (R√©gularisation)
  On constate qu'en augmentant le  goupss la convergence ralentie, on sur-r√©gularise, ce qui bride le mod√®le et la fait perdre en accuracy.

---

## 6) Entra√Ænement complet (10‚Äì20 √©poques, sans scheduler)

- **Configuration finale** :
  - LR = `_____`
  - Weight decay = `_____`
  - Hyperparam√®tre mod√®le A = `_____`
  - Hyperparam√®tre mod√®le B = `_____`
  - Batch size = `_____`
  - √âpoques = `_____` (10‚Äì20)
- **Checkpoint** : `artifacts/best.ckpt` (selon meilleure m√©trique val)

> _Ins√©rer captures TensorBoard :_
>
> - `train/loss`, `val/loss`
> - `val/accuracy` **ou** `val/f1` (classification)

**M6.** Montrez les **courbes train/val** (loss + m√©trique). Interpr√©tez : sous-apprentissage / sur-apprentissage / stabilit√© d‚Äôentra√Ænement.

---

## 7) Comparaisons de courbes (analyse)

> _Superposez plusieurs runs dans TensorBoard et ins√©rez 2‚Äì3 captures :_

- **Variation du LR** (impact au d√©but d‚Äôentra√Ænement)
- **Variation du weight decay** (√©cart train/val, r√©gularisation)
- **Variation des 2 hyperparam√®tres de mod√®le** (convergence, plateau, surcapacit√©)

**M7.** Trois **comparaisons** comment√©es (une phrase chacune) : LR, weight decay, hyperparam√®tres mod√®le ‚Äî ce que vous attendiez vs. ce que vous observez.

---

## 8) It√©ration suppl√©mentaire (si temps)

- **Changement(s)** : `_____` (resserrage de grille, nouvelle valeur d‚Äôun hyperparam√®tre, etc.)
- **R√©sultat** : `_____` (val metric, tendances des courbes)

**M8.** D√©crivez cette it√©ration, la motivation et le r√©sultat.

---

## 9) √âvaluation finale (test)

- **Checkpoint √©valu√©** : `artifacts/best.ckpt`
- **M√©triques test** :
  - Metric principale (nom = `_____`) : `_____`
  - Metric(s) secondaire(s) : `_____`

**M9.** Donnez les **r√©sultats test** et comparez-les √† la validation (√©cart raisonnable ? surapprentissage probable ?).

---

## 10) Limites, erreurs & bug diary (court)

- **Limites connues** (donn√©es, compute, mod√®le) :
- **Erreurs rencontr√©es** (shape mismatch, divergence, NaN‚Ä¶) et **solutions** :
- **Id√©es ¬´ si plus de temps/compute ¬ª** (une phrase) :

---

## 11) Reproductibilit√©

- **Seed** : `_____`
- **Config utilis√©e** : joindre un extrait de `configs/config.yaml` (sections pertinentes)
- **Commandes exactes** :

```bash
# Exemple (remplacer par vos commandes effectives)
python -m src.train --config configs/config.yaml --max_epochs 15
python -m src.evaluate --config configs/config.yaml --checkpoint artifacts/best.ckpt
````

- **Artifacts requis pr√©sents** :

  - [ ] `runs/` (runs utiles uniquement)
  - [ ] `artifacts/best.ckpt`
  - [ ] `configs/config.yaml` align√© avec la meilleure config

---

## 12) R√©f√©rences (courtes)

- PyTorch docs des modules utilis√©s (Conv2d, BatchNorm, ReLU, LSTM/GRU, transforms, etc.).
- Lien dataset officiel (et/ou HuggingFace/torchvision/torchaudio).
- Toute ressource externe substantielle (une ligne par r√©f√©rence).
